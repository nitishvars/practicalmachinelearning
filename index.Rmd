---
title: "Practical Machine Learning Assignment"
author: "Nitish Varshney"
date: "August 22, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Practical Machine Learning Prediction Assignment Writeup

This is Prediction Assignment Writeup. For problem statement and Data sources, see README.md.

**Goal** To predict the manner in which 6 participants did the exercise. This is the "classe" variable in the training set. One should create a report describing how (s)he built model, how (s)he used cross validation, what (s)he think the expected out of sample error is, and why (s)he made the choices for a classifer.

### Model building

**Libraries Used**
```{r caret}
library(caret)
```

**Loading the Data** from the Dataset
```{r loadingTheData,cache=TRUE}
trainingData <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"), header = TRUE, sep = ",")
testingData <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"), header = TRUE, sep = ",")
```

**Processing the Data**

<i>Removing id variable</i>
```{r}
trainingData <- trainingData[c(-1)]
testingData <- testingData[c(-1)]
```

<i>On looking at the training data, you can see few column values is "#DIV/0!"</i>
```{r, warning=FALSE}
trainingData[trainingData =="#DIV/0!"] <- 0
```

<i>Finding if there exists near-zero variance predictors in the data set.</i>

```{r,echo=FALSE}
options(width=130)
```

```{r nearZeroVariableExistanceFinder,cache=TRUE}
nsv <- nearZeroVar(trainingData)
names(trainingData)[nsv]
```

<i>As there exists near-zero variance predictors, let us remove them
```{r removeNSV}
trainingData <- trainingData [-nsv]
testingData <- testingData[-nsv]
```

<i>Discarding Features with many missing values</i>
```{r,echo=FALSE}
options(width=80)
```

```{r}
##Only a subset of trainingData is shown below, for better viewing purpose only.
summary(trainingData[,c(10:16)])
```

<i>As there exists NAs in the data set, let us omit all those variables which have very high NAs</i>
```{r}
##Getting index of all the variables having high number of NAs
myNAVars <- names(trainingData) %in% c("max_picth_belt","min_roll_belt","max_roll_belt","min_pitch_belt","amplitude_roll_belt","amplitude_pitch_belt","var_total_accel_belt","avg_roll_belt","stddev_roll_belt","var_roll_belt","avg_pitch_belt","stddev_pitch_belt","var_pitch_belt","avg_yaw_belt","stddev_yaw_belt","var_yaw_belt","var_accel_arm","max_picth_arm","max_yaw_arm","min_yaw_arm","amplitude_yaw_arm","max_roll_dumbbell","max_picth_dumbbell","min_roll_dumbbell","min_pitch_dumbbell","amplitude_roll_dumbbell","amplitude_pitch_dumbbell","var_accel_dumbbell","avg_roll_dumbbell","stddev_roll_dumbbell","var_roll_dumbbell","avg_pitch_dumbbell","stddev_pitch_dumbbell","var_pitch_dumbbell","avg_yaw_dumbbell","stddev_yaw_dumbbell","var_yaw_dumbbell","max_picth_forearm","min_pitch_forearm","amplitude_pitch_forearm","var_accel_forearm")
trainingData<-trainingData[,!myNAVars]
testingData<-testingData[,!myNAVars]
```

<i>On further looking at the data, some variable seems to be descriptive only, with no relevance to be predication</i>
```{r}
myDescVars <- names(trainingData) %in% c("user_name", "raw_timestamp_part_1", "raw_timestamp_part_2",    "cvtd_timestamp", "new_window", "num_window")
trainingData<-trainingData[,!myDescVars]
testingData<-testingData[,!myDescVars]
```

### Cross-Validation

Cross-Validation has been performed by partioning training data set into 2 parts,
Training (70% of the original dataset), Validation (30% of the dataset). We will train various models on Training dataset and test on the Validation dataset. Model having best accuracy will be tested on testingData.

```{r, cache=TRUE}
#Creating indices for the 2 dataset 
set.seed(1499)
inTrain <- createDataPartition(y=trainingData$classe,p=0.8,list = FALSE)
Training <- trainingData[inTrain, ]
Validation <- trainingData[-inTrain, ]
```

```{r cars}
dim(trainingData)
dim(Training)
dim(Validation)
dim(testingData)
```

### Building Various Models

<i>As after Processing the data, variables left have only numeric values. So, Decision Tree or Random Forest algorithm should work fine.</i><br/>
<i>Using Decision Tree algorithm for prediction</i>
```{r decisionTreeTrain, cache=TRUE}
#### Due to Unknown reason train method was giving 48% accuracy
#modFitDT <- train(classe ~., method="rpart",data=Training)
#library(rattle)
#fancyRpartPlot(modFitDT$finalModel)
library(rpart)
modFitDT <- rpart(classe ~., method="class", data=Training)
library(rpart.plot)
# So many levels were there so image was not coming properly
#rpart.plot(modFitDT, main="Classification Tree")
print(modFitDT)
predictionsDT <- predict(modFitDT, Validation, type="class")
confusionMatrix(predictionsDT , Validation$classe)
```

<i>Using Random Forest algorithm for prediction</i>
```{r randonForestTrain, cache=TRUE}
library("randomForest")
modFitRF <- randomForest(classe ~., method="class", data=Training)
print(modFitRF)
predictionsRF <- predict(modFitRF, Validation, type="class")
confusionMatrix(predictionsRF , Validation$classe)
```

### Final Model Selection
As expected Random Forest Algorithm has performed better than Decision Tree Algorithm (Random Forest Algorithm Accuracy was 99.54% in comparision to Decision Tree Algorithm Accuracy 74.05%). So we choose Random Forest model as final model.

### Expected out of Sample Error
The expected out-of-sample error is calculated as 1 - accuracy for predictions made against the cross-validation set. Our Test data set comprises 20 cases. With an accuracy above 99% on our cross-validation data by random forest model, we can expect that very few, or none, of the test samples will be missclassified.


### Test Prediction Results
```{r testPrediction}
predictionsFinal <- predict(modFitRF, testingData)
predictionsFinal
```